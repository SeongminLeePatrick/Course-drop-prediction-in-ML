{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extration (using KDD '15 Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import datetime\n",
    "\n",
    "### Zeppelin automatically binds spark variables to its interpreter.\n",
    "### Thus no need to run the following line.\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "def log2row(line):    \n",
    "    fields = line.split(',')\n",
    "    row_eid = int(fields[0])\n",
    "    row_source = fields[2]\n",
    "    row_event = fields[3]\n",
    "    row_object = fields[4]\n",
    "    t = fields[1].split('T')\n",
    "    r_date = t[0].split('-')\n",
    "    r_time = t[1].split(':')\n",
    "    row_date = datetime.datetime(int(r_date[0]), int(r_date[1]), int(r_date[2]), 0, 0, 0)\n",
    "    row_time = datetime.datetime(1990, 1, 1, int(r_time[0]), int(r_time[1]), int(r_time[2]))\n",
    "    return Row(enrollment_id = row_eid, date = row_date, time = row_time, source = row_source, event = row_event, object_id = row_object )\n",
    "\n",
    "\n",
    "def true2row(line):\n",
    "    fields = line.split(',')\n",
    "    row_eid = int(fields[0])\n",
    "    row_dropout = bool(int(fields[1]))\n",
    "    return Row(enrollment_id = row_eid, dropout = row_dropout)\n",
    "        \n",
    "\n",
    "def date2row(line):\n",
    "    fields = line.split(',')\n",
    "    row_cid = fields[0]\n",
    "    r_from = fields[1].split('-')\n",
    "    r_to = fields[2].split('-')\n",
    "    row_from = datetime.datetime(int(r_from[0]), int(r_from[1]), int(r_from[2]), 0, 0, 0)\n",
    "    row_to = datetime.datetime(int(r_to[0]), int(r_to[1]), int(r_to[2]), 0, 0, 0)    \n",
    "    return Row(course_id = row_cid, fromdate = row_from, todate = row_to) \n",
    "\n",
    "def enrollment2row(line):\n",
    "    fields = line.split(',')\n",
    "    row_eid = int(fields[0])\n",
    "    row_username = fields[1]\n",
    "    row_cid = fields[2]\n",
    "    return Row(enrollment_id = row_eid, username = row_username, course_id = row_cid)\n",
    "  \n",
    "    \n",
    "def object2row(line):\n",
    "    fields = line.split(',')\n",
    "    if len(fields) == 5:    \n",
    "        row_cid = fields[0]\n",
    "        row_mid = fields[1]\n",
    "        row_category = fields[2]\n",
    "        row_children = fields[3]\n",
    "        row_date = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        row_time = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        if fields[4] != \"null\":\n",
    "            t = fields[4].split('T')\n",
    "            r_date = t[0].split('-')\n",
    "            r_time = t[1].split(':')\n",
    "            row_date = datetime.datetime(int(r_date[0]), int(r_date[1]), int(r_date[2]), 0, 0, 0)\n",
    "            row_time = datetime.datetime(1990, 1, 1, int(r_time[0]), int(r_time[1]), int(r_time[2]))\n",
    "        return Row(course_id = row_cid, module_id = row_mid, category = row_category, children=row_children, date=row_date, time=row_time)\n",
    "    elif len(fields) == 4:\n",
    "        row_cid = fields[0]\n",
    "        row_mid = fields[1]\n",
    "        row_category = fields[2]\n",
    "        row_children = \"\"        \n",
    "        row_date = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        row_time = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        if fields[3] != \"null\":\n",
    "            t = fields[3].split('T')\n",
    "            r_date = t[0].split('-')\n",
    "            r_time = t[1].split(':')\n",
    "            row_date = datetime.datetime(int(r_date[0]), int(r_date[1]), int(r_date[2]), 0, 0, 0)\n",
    "            row_time = datetime.datetime(1990, 1, 1, int(r_time[0]), int(r_time[1]), int(r_time[2]))\n",
    "        return Row(course_id = row_cid, module_id = row_mid, category = row_category, children=row_children, date=row_date, time=row_time)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#Convert each csv file to RDD\n",
    "#Parse each line using function defined above\n",
    "#Convert parsed RDD to DataFrame and create temp views\n",
    "\n",
    "\n",
    "###########################################################\n",
    "############ TODO change it to your directory #############\n",
    "DATAPATH = \"file:///home/ss00/5_Feature_Extraction/featuredata\"\n",
    "###########################################################\n",
    "\n",
    "\n",
    "log_table = spark.sparkContext.textFile(\"{}/log_10percent.csv\".format(DATAPATH)) # use truncated dataset (10%).\n",
    "log_head = log_table.first()\n",
    "log_table_no_header = log_table.filter(lambda x : x != log_head)\n",
    "\n",
    "\n",
    "true_table = spark.sparkContext.textFile(\"{}/truth_train.csv\".format(DATAPATH))\n",
    "true_head = true_table.first()\n",
    "true_table_no_header = true_table.filter(lambda x : x != true_head)\n",
    "\n",
    "\n",
    "date_table = spark.sparkContext.textFile(\"{}/date.csv\".format(DATAPATH))\n",
    "date_head = date_table.first()\n",
    "date_table_no_header = date_table.filter(lambda x : x != date_head)\n",
    "\n",
    "\n",
    "enrollment_table = spark.sparkContext.textFile(\"{}/enrollment.csv\".format(DATAPATH))\n",
    "enrollment_head = enrollment_table.first()\n",
    "enrollment_table_no_header = enrollment_table.filter(lambda x : x != enrollment_head)\n",
    "\n",
    "object_table = spark.sparkContext.textFile(\"{}/object.csv\".format(DATAPATH))\n",
    "object_head = object_table.first()\n",
    "object_table_no_header = object_table.filter(lambda x : x != object_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "log_rows = log_table_no_header.map(log2row)\n",
    "true_rows = true_table_no_header.map(true2row)\n",
    "date_rows = date_table_no_header.map(date2row)\n",
    "enrollment_rows = enrollment_table_no_header.map(enrollment2row)\n",
    "object_rows = object_table_no_header.map(object2row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "log_df = spark.createDataFrame(log_rows).cache()\n",
    "log_df.createOrReplaceTempView(\"log_t\")\n",
    "\n",
    "true_df = spark.createDataFrame(true_rows).cache()\n",
    "true_df.createOrReplaceTempView(\"true_t\")\n",
    "\n",
    "date_df = spark.createDataFrame(date_rows).cache()\n",
    "date_df.createOrReplaceTempView(\"date_t\")\n",
    "\n",
    "enrollment_df = spark.createDataFrame(enrollment_rows).cache()\n",
    "enrollment_df.createOrReplaceTempView(\"enrollment_t\")\n",
    "\n",
    "object_df = spark.createDataFrame(object_rows).cache()\n",
    "object_df.createOrReplaceTempView(\"object_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "log_df.show(3)\n",
    "log_df.printSchema()\n",
    "true_df.show(3)\n",
    "date_df.show(3)\n",
    "enrollment_df.show(3)\n",
    "object_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT enrollment_id,\n",
    "sum(CASE WHEN event = \"discussion\" THEN count_event ELSE 0 END) c_discusion,\n",
    "sum(CASE WHEN event = \"wiki\" THEN count_event ELSE 0 END) c_wiki,\n",
    "sum(CASE WHEN event = \"page_close\" THEN count_event ELSE 0 END) c_page_close,\n",
    "sum(CASE WHEN event = \"access\" THEN count_event ELSE 0 END) c_access,\n",
    "sum(CASE WHEN event = \"video\" THEN count_event ELSE 0 END) c_video,\n",
    "sum(CASE WHEN event = \"navigate\" THEN count_event ELSE 0 END) c_navigate,\n",
    "sum(CASE WHEN event = \"problem\" THEN count_event ELSE 0 END) c_problem \n",
    "FROM \n",
    " (SELECT enrollment_id, event, count(*) as count_event\n",
    " FROM log_t \n",
    " group by enrollment_id, event) \n",
    " group by enrollment_id \n",
    " order by enrollment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 1\n",
    "# COUNT: 7\n",
    "results1 = spark.sql('''SELECT enrollment_id,\n",
    "sum(CASE WHEN event = \"discussion\" THEN count_event ELSE 0 END) c_discusion,\n",
    "sum(CASE WHEN event = \"wiki\" THEN count_event ELSE 0 END) c_wiki,\n",
    "sum(CASE WHEN event = \"page_close\" THEN count_event ELSE 0 END) c_page_close,\n",
    "sum(CASE WHEN event = \"access\" THEN count_event ELSE 0 END) c_access,\n",
    "sum(CASE WHEN event = \"video\" THEN count_event ELSE 0 END) c_video,\n",
    "sum(CASE WHEN event = \"navigate\" THEN count_event ELSE 0 END) c_navigate,\n",
    "sum(CASE WHEN event = \"problem\" THEN count_event ELSE 0 END) c_problem \n",
    "FROM \n",
    " (SELECT enrollment_id, event, count(*) as count_event\n",
    " FROM log_t \n",
    " group by enrollment_id, event) \n",
    " group by enrollment_id \n",
    " order by enrollment_id ''').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT enrollment_id,\n",
    "sum(CASE WHEN source = \"browser\" THEN count_event ELSE 0 END) s_browser,\n",
    "sum(CASE WHEN source = \"server\" THEN count_event ELSE 0 END) s_server\n",
    "FROM \n",
    " (SELECT enrollment_id,source, count(*) as count_event FROM log_t group by enrollment_id, source order by enrollment_id)\n",
    " group by enrollment_id\n",
    " order by enrollment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 2\n",
    "# COUNT: 2, ACCUMULATE: 9\n",
    "results2 = spark.sql('''SELECT enrollment_id,\n",
    "sum(CASE WHEN source = \"browser\" THEN count_event ELSE 0 END) s_browser,\n",
    "sum(CASE WHEN source = \"server\" THEN count_event ELSE 0 END) s_server\n",
    "FROM \n",
    " (SELECT enrollment_id,source, count(*) as count_event FROM log_t group by enrollment_id, source order by enrollment_id)\n",
    " group by enrollment_id\n",
    " order by enrollment_id\n",
    " ''').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Join SQL 2 with 1\n",
    "joined_results1_2 = results2.join(results1,'enrollment_id','outer').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT course_id, count(enrollment_id) as count_eid_per_course FROM enrollment_t group by course_id order by course_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 3\n",
    "# COUNT: 1, ACCUMULATE: 10\n",
    "results3 = spark.sql('SELECT course_id, count(enrollment_id) as count_eid_per_course FROM enrollment_t group by course_id order by course_id').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT course_id, count(e.enrollment_id) as dropout_per_course FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by course_id order by course_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 4\n",
    "# COUNT: 1, ACCUMULATE: 11\n",
    "results4 = spark.sql('SELECT course_id, count(e.enrollment_id) as dropout_per_course FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by course_id order by course_id').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT sq1.course_id as course_id, sq2.do/sq1.ce as dropoutrate_per_course\n",
    "FROM (SELECT course_id, count(e.enrollment_id) as ce FROM enrollment_t as e, true_t as t\n",
    "        WHERE e.enrollment_id = t.enrollment_id group by course_id) as sq1,\n",
    "     (SELECT course_id, count(dropout) as do FROM enrollment_t as e, true_t as t\n",
    "        WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by course_id) as sq2\n",
    "WHERE sq1.course_id = sq2.course_id order by sq1.course_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 5\n",
    "# COUNT: 1, ACCUMULATE: 12\n",
    "results5 = spark.sql('SELECT sq1.course_id as course_id, sq2.do/sq1.ce as dropoutrate_per_course FROM (SELECT course_id, count(e.enrollment_id) as ce FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id group by course_id) as sq1, (SELECT course_id, count(dropout) as do FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by course_id) as sq2 WHERE sq1.course_id = sq2.course_id order by sq1.course_id').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Join SQL 3,4,5 with 1~2\n",
    "joined_result_3_4_5 = results3.join(results4,'course_id','outer').join(results5,'course_id','outer').cache()\n",
    "joined_result_3_4_5_edf = joined_result_3_4_5.join(enrollment_df, 'course_id','outer').cache()\n",
    "joined_result1_2_3_4_5_edf = joined_result_3_4_5_edf.join(joined_results1_2, 'enrollment_id','outer').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT username, count(enrollment_id) as count_eid_per_user FROM enrollment_t group by username order by username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#SQL 6\n",
    "# COUNT: 1, ACCUMULATE: 13\n",
    "results6 = spark.sql('SELECT username, count(enrollment_id) as count_eid_per_user FROM enrollment_t group by username order by username').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT e.username as username, count(e.enrollment_id) as dropout_per_user FROM true_t as t, enrollment_t as e WHERE t.enrollment_id = e.enrollment_id and t.dropout = 1 group by e.username order by e.username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#SQL 7\n",
    "# COUNT: 1, ACCUMULATE: 14\n",
    "results7 = spark.sql('SELECT e.username as username, count(e.enrollment_id) as dropout_per_user FROM true_t as t, enrollment_t as e WHERE t.enrollment_id = e.enrollment_id and t.dropout = 1 group by e.username order by e.username').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT sq1.username as username , sq2.do/sq1.ce as dropoutrate_per_user\n",
    "FROM (SELECT username, count(e.enrollment_id) as ce FROM enrollment_t as e, true_t as t\n",
    "        WHERE e.enrollment_id = t.enrollment_id group by username) as sq1, \n",
    "     (SELECT username, count(dropout) as do FROM enrollment_t as e, true_t as t\n",
    "        WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by username) as sq2\n",
    "WHERE sq1.username = sq2.username order by sq1.username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 8\n",
    "# COUNT: 1, ACCUMULATE: 15\n",
    "results8 = spark.sql('SELECT sq1.username as username , sq2.do/sq1.ce as dropoutrate_per_user FROM (SELECT username, count(enrollment_id) as ce FROM enrollment_t group by username) as sq1,(SELECT username, count(dropout) as do FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by username) as sq2 WHERE sq1.username = sq2.username order by sq1.username').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Join SQL 6,7,8 with 1~5\n",
    "joined_result_6_7_8 = results6.join(results7,'username','outer').join(results8, 'username','outer').cache()\n",
    "joined_result1_2_3_4_5_6_7_8_edf = joined_result_6_7_8.join(joined_result1_2_3_4_5_edf, 'username','outer').drop('username').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT e.enrollment_id, datediff( MAX(l.date), MIN(l.date) ) as period FROM log_t as l, enrollment_t  as e WHERE e.enrollment_id = l.enrollment_id group by e.enrollment_id order by e.enrollment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#SQL 9 \n",
    "# COUNT: 1, ACCUMULATE: 16\n",
    "results9 = spark.sql('SELECT e.enrollment_id, datediff( MAX(l.date), MIN(l.date) ) as period FROM log_t as l, enrollment_t  as e WHERE e.enrollment_id = l.enrollment_id group by e.enrollment_id order by e.enrollment_id').cache()\n",
    "\n",
    "# Uncomment and visualize\n",
    "#z.show(results9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Join SQL 9 with 1~8\n",
    "joined_result1_2_3_4_5_6_7_8_9_edf = results9.join(joined_result1_2_3_4_5_6_7_8_edf, 'enrollment_id','outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT  e.enrollment_id, count(distinct(l.date)) as effective_study_days FROM log_t as l, enrollment_t as e WHERE e.enrollment_id = l.enrollment_id group by e.enrollment_id order by e.enrollment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# SQL 10\n",
    "# COUNT: 1, ACCUMULATE: 17\n",
    "results10 = spark.sql('SELECT  e.enrollment_id, count(distinct(l.date)) as effective_study_days FROM log_t as l, enrollment_t as e WHERE e.enrollment_id = l.enrollment_id group by e.enrollment_id order by e.enrollment_id').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Join SQL 10 with 1~9\n",
    "joined_result1_2_3_4_5_6_7_8_9_10_edf = results10.join(joined_result1_2_3_4_5_6_7_8_9_edf, 'enrollment_id','outer').cache()\n",
    "joined_result1_2_3_4_5_6_7_8_9_10_edf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joined_result_total = joined_result1_2_3_4_5_6_7_8_9_10_edf.drop('course_id')\n",
    "joined_result_total_numeric = joined_result_total.na.fill(0)\n",
    "\n",
    "agg_result = joined_result_total_numeric.join(true_df, 'enrollment_id', how='left_outer').cache()\n",
    "\n",
    "# Split\n",
    "train_dataset_df = agg_result.filter('dropout is  not null').sort('enrollment_id').cache()\n",
    "test_dataset_df = agg_result.filter('dropout is null').sort('enrollment_id').drop('dropout').cache()\n",
    "agg_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(joined_result1_2_3_4_5_6_7_8_9_10_edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"The number of dropout: \", train_dataset_df.filter(train_dataset_df['dropout'] == True).count())\n",
    "print(\"The number of NOT dropout: \", train_dataset_df.filter(train_dataset_df['dropout'] == False).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True, inputCol=\"temp_feature\", outputCol=\"scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "train_dataset_df = train_dataset_df.withColumn('label', train_dataset_df['dropout'].cast(types.IntegerType()))\n",
    "train_dataset_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "BalancingRatio=  train_dataset_df.filter(train_dataset_df['label'] == 1).count() / train_dataset_df.count() #num_Majority / dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "train_dataset_df = train_dataset_df.withColumn(\"classWeights\", F.when(train_dataset_df['label'] == 0, BalancingRatio).otherwise(1 - BalancingRatio))\n",
    "train_dataset_df.select(\"label\",\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "categoricalColumns=[]\n",
    "stages=[]\n",
    "\n",
    "for c in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = c, outputCol=c+'index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols = [stringIndexer.getOutputCol()], outputCols =[c + 'classVec'])\n",
    "    stages += [stringIndexer, encoder ]\n",
    "    \n",
    "labelIndexer = StringIndexer(inputCol = 'label',outputCol = 'indexedLabel').setHandleInvalid(\"skip\")\n",
    "stages += [labelIndexer]\n",
    "\n",
    "list_of_col_not_numeric = categoricalColumns + ['enrollment_id', 'label','dropout']\n",
    "numericCols = agg_result.drop(*list_of_col_not_numeric).columns\n",
    "\n",
    "assemblerInputs = [c  + 'classVec' for c in cateCol] + numericCols\n",
    "vectorAssembler = VectorAssembler(inputCols  = assemblerInputs, outputCol=\"Features\")\n",
    "\n",
    "stages+= [vectorAssembler]\n",
    "\n",
    "scaler = StandardScaler(inputCol = 'Features', outputCol ='scaledFeatures', withStd = True, withMean= False)\n",
    "\n",
    "stages +=[scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol = \"indexedLabel\", featuresCol= \"scaledFeatures\", weightCol = \"classWeights\" )\n",
    "rf = RandomForestClassifier(labelCol = \"indexedLabel\", featuresCol = \"scaledFeatures\")\n",
    "\n",
    "lr_params = ParamGridBuilder().addGrid(lr.regParam, [0.1]).addGrid(lr.maxIter, [300]).build()\n",
    "                                # .addGrid(lr.elasticNetParam, [0.1, 0.5, 1.0])\\\n",
    "                                \n",
    "                                \n",
    "\n",
    "rf_params = ParamGridBuilder().addGrid(rf.maxDepth, [5, 10])\\\n",
    "                                .addGrid(rf.numTrees, [200, 500]).build()\n",
    "                                # .addGrid(rf.featureSubsetStrategy, ['sqrt'])\n",
    "                                \n",
    "                                \n",
    "binary_evaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC') #option, areaUnderPR, areaUnderROC\n",
    "lr_cv = CrossValidator(estimator = lr, estimatorParamMaps = lr_params, evaluator = binary_evaluator,  numFolds = 10, seed = 900)\n",
    "rf_cv = CrossValidator(estimator = rf, estimatorParamMaps = rf_params, evaluator = binary_evaluator,  numFolds = 10, seed = 900)\n",
    "\n",
    "stages_lr_cv= stages + [lr_cv]\n",
    "stages_rf_cv= stages + [rf_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "prepPipeline = Pipeline().setStages(stages_lr_cv)\n",
    "pipelineModel = prepPipeline.fit(train_dataset_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "test_dataset_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "pipelineModel.transform(test_dataset_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "second_element = udf(lambda v:float(v[1]), FloatType())\n",
    "\n",
    "prediction = pipelineModel.transform(test_dataset_df)\n",
    "prediction = prediction.withColumn(\"proba_as_1\", second_element('probability'))\n",
    "prediction.select(\"enrollment_id\", \"proba_as_1\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
